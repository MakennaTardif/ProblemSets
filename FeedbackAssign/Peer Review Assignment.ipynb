{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2f656d-410e-4e49-b52d-ade725ac4457",
   "metadata": {},
   "source": [
    "## **(1) Big picture of each script** ##\n",
    "\n",
    "- **My script:**\n",
    "\n",
    "> Makes a full-screen white window, loads T and L images, shows a fixation, and presents instructions with explicit keys 1 = present and 0 = absent.\n",
    "\n",
    "> Then practice (untimed with feedback), then main trials (timed with a Clock), and saves results at the end of the experiment.\n",
    "\n",
    "> The coordinate maps are pre-centered for set sizes 3/6/9, which makes layouts predictable and visually clean.\n",
    "\n",
    "- **Classmates script:**\n",
    "\n",
    "> Starts with a GUI dialog to grab SubjectID and trials per condition and immediately opens a CSV with a header (so every run is labelled).\n",
    "\n",
    "> Uses functions for placing stimuli (pos_and_ori), deciding present/absent (targ_pres), and response collection (KeyGet).\n",
    "\n",
    "> Then runs practice + main blocks and shows a final summary screen (average RT, accuracy, and number of misses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c0bb7-f48d-483e-bd4e-33bdb8a589ea",
   "metadata": {},
   "source": [
    "## **(2) What I liked/learned from my classmates script + some things to change** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373978a-4dc6-44f2-94e9-3231513ca39e",
   "metadata": {},
   "source": [
    "#### **(A) Quick startup using GUI + immediately opening a labeled CSV** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa0704-44a1-4452-8443-f3563e9795b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlg = gui.Dlg()\n",
    "dlg.addField('SubjectID:')\n",
    "dlg.addField('Trials Per Cond:')\n",
    "ok_data = dlg.show()\n",
    "if not dlg.OK:\n",
    "    core.quit()\n",
    "\n",
    "sub_ID = ok_data[0]\n",
    "trials = int(dlg.data[1])\n",
    "fileName = sub_ID + \"_\" + expName\n",
    "dataFile = open(fileName + '.csv', 'w')\n",
    "dataFile.write('SetSize,TP, RT, Correct, Missed\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45994e-e411-4648-abb2-d834363145e6",
   "metadata": {},
   "source": [
    "- **I like this approach because it makes the script interactive and flexible.**\n",
    "\n",
    "> Instead of hardcoding the participant IDs or number of trials, this uses gui.Dlg() to collect those values when the experiment starts.\n",
    "\n",
    "> It’s a small change that makes the code easier to reuse for different participants without editing the script every time.\n",
    "\n",
    "- **It also sets up the data file right away, with a header line.**\n",
    "\n",
    "> That’s really smart for data management, because it means each participant automatically gets their own CSV labelled with their Subject ID (from fileName = sub_ID + \"_\" + expName).\n",
    "\n",
    "> If the script crashes or is stopped mid-run, you would still have at least a partial dataset with proper column names, which is really helpful during debugging or pilot testing.\n",
    "\n",
    "> This is something that I did not do in my own script, but I can see the importance now (i.e., if the experiment crashes/stopped, I would lose all my data).\n",
    "\n",
    "- **The structure makes the “save pathway” visible early in the code.**\n",
    "\n",
    "> I can tell exactly where data is being saved and under what name, which helps prevent overwriting previous runs.\n",
    "\n",
    "> I like that it’s formatted as 'SetSize,TP, RT, Correct, Missed\\n'—those headers directly match the variables that get saved later, which will make imports to things like SPSS or Excel easy.\n",
    "\n",
    "- **Tiny improvement (what I learned to consider):**\n",
    "\n",
    "> **I might wrap the file creation line in a context manager:**\n",
    "\n",
    "> with open(fileName + '.csv', 'w') as dataFile:\n",
    "          dataFile.write('SetSize,TP, RT, Correct, Missed\\n')\n",
    "\n",
    "> That way, the file will automatically close even if PsychoPy crashes or the experimenter ends early.\n",
    "\n",
    "> It’s a small thing, but it’s considered best practice for file safety.\n",
    "\n",
    "- **Broader takeaway I learned:**\n",
    "\n",
    "> This snippet of code reminded me that collecting participant info and setting up data output at the top of a PsychoPy script helps organize the whole experiment flow.\n",
    "\n",
    "> It keeps participant-specific setup separate from the trial logic, and that makes debugging easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428af51-6fe0-4166-9015-47034580b553",
   "metadata": {},
   "source": [
    "#### **(B) Randomized positions + orientations helper (more compact stimulus drawing)** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e84b2d-26ae-48da-b2ba-e1d59171bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_and_ori(target, distract, samp_size):\n",
    "    samplelist = list(range(-180, 180, 25))\n",
    "    x = random.sample(samplelist, samp_size)\n",
    "    y = random.sample(samplelist, samp_size)\n",
    "    for n in range(0, samp_size - 1):\n",
    "        orientations = [0, 90, 180, 270]\n",
    "        orin = random.choice(orientations)\n",
    "        distract.ori = orin\n",
    "        distract.pos = (x[n], y[n])\n",
    "        distract.draw()\n",
    "    for n in range(samp_size - 1, samp_size):\n",
    "        target.pos = (x[n], y[n])\n",
    "        target.draw()\n",
    "    return distract, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de5fed-b61e-44a9-b409-015cce796c7f",
   "metadata": {},
   "source": [
    "- **What this function does:**\n",
    "\n",
    "> This function neatly bundles all the “draw stimuli on the screen” steps into one reusable piece of code.\n",
    "\n",
    "> It randomly selects a set of (x, y) coordinates, assigns each distractor (the Ls) a random orientation, and then draws exactly one target (the T) in the final coordinate slot.\n",
    "\n",
    "> By wrapping this in a function, the main loop becomes much shorter — I just call pos_and_ori() instead of repeating the same drawing logic for every trial.\n",
    "\n",
    "> It’s a really efficient way to keep the main experiment flow clean.\n",
    "\n",
    "- **What I liked/learned:**\n",
    "\n",
    "> I like how this uses two loops so that the last item automatically becomes the target.\n",
    "\n",
    "> That’s a simple but smart way to separate distractors and the target without needing a “target index” variable like I included in my own script.\n",
    "\n",
    "> I also learned that using random.sample() avoids duplicate positions, reducing the chance that shapes overlap or cluster.\n",
    "\n",
    "- **Two things to keep in mind (both myself and my classmate):**\n",
    "\n",
    "> **(1)** This code assumes exactly one target per trial and fills the final slot with that target. That’s perfect for a classic visual search task like this, but if I wanted to manipulate target frequency or have multiple targets, you would need to change the logic that decides how many distractors vs. targets are drawn.\n",
    "\n",
    "> **(2)** Because the x and y coordinates are sampled independently, it’s possible that some positions line up too neatly (e.g., a grid-like pattern). If you wanted a more controlled spacing (equal distance between items), I might generate coordinates from a grid and randomly shuffle them instead of sampling x and y separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acfde79-0d42-4ef1-a02d-fd4487beb53d",
   "metadata": {},
   "source": [
    "#### **(C) Clear, self-contained “present vs absent” decision (single entry point)** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a28647-7311-4d8c-a8ac-f6b83b16b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targ_pres(trial_list, total_trials, distract, target, condition_index):\n",
    "    pres_or_not = random.choice(trial_list)\n",
    "    trial_list.remove(pres_or_not)\n",
    "    if pres_or_not <= np.median(total_trials):\n",
    "        targ_there = 0\n",
    "        stimuli = pos_and_ori(distract, distract, condition)\n",
    "    else:\n",
    "        targ_there = 1\n",
    "        stimuli = pos_and_ori(target, distract, condition)\n",
    "    return targ_there, stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59df22-9c49-4771-bc5f-497f895e6d14",
   "metadata": {},
   "source": [
    "- **As a beginner, I appreciate a single function that decides present/absent and immediately draws the correct array.**\n",
    "\n",
    "> I really like that this function handles both deciding whether the target is present and drawing the correct array right away.\n",
    "\n",
    "> It keeps all of the logic for presence/absence in one place, which reduces the number of moving parts in the main loop.\n",
    "\n",
    "> I didn’t do this in my own code because I wasn’t sure how to combine those steps, but seeing it here helped me understand how efficient and organized this structure can be.\n",
    "\n",
    "- **I also learned a few interesting tricks:**\n",
    "\n",
    "> The use of the median to split the list into half “present” and half “absent” trials is clever and compact. It automatically balances the number of each trial type across a block without needing two separate lists.\n",
    "\n",
    ">Another thing I learned is how they call pos_and_ori(distract, distract) for absent trials and pos_and_ori(target, distract) for present trials. It’s a short, elegant way to make sure a target is only drawn when it should be.\n",
    "\n",
    "- **Something to consider changing:**\n",
    "\n",
    "> Even though this method works well, it’s a bit hard to read at first glance (i.e., I was quite confused for a few minutes).\n",
    "\n",
    "> From my understanding, it relies on knowing how the trial list and median are used, which might confuse newer coders (aka me).\n",
    "\n",
    "> A more explicit flag for “present = True/False” could make this even clearer.\n",
    "\n",
    "> **What I may do:** Possible build a list of presence flags (e.g., [0,1,0,1,...]), shuffle it, and in the main loop do present = flags[i]; then call a draw helper with present=True/False. It’s a tiny bit longer, but to me easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28931b20-8b77-4a0c-8929-008f161b4173",
   "metadata": {},
   "source": [
    "#### **(D) Response collection with a per-trial timeout (beginner-friendly loop)** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31a36d-15cb-47a3-a0c5-35d2c716ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KeyGet(trial_duration=2.0, rt=None, resp=None):\n",
    "    startTime = core.getTime()\n",
    "    while core.getTime() - startTime < trial_duration and resp is None:\n",
    "        keys = event.getKeys(keyList=['a', 'd', 'escape'])\n",
    "        if keys:\n",
    "            key = keys[0]\n",
    "            rt = core.getTime() - startTime\n",
    "            if key == 'a':\n",
    "                resp = 'a'\n",
    "                break\n",
    "            elif key == 'd':\n",
    "                resp = 'd'\n",
    "                break\n",
    "            elif key == 'escape':\n",
    "                core.quit()\n",
    "        core.wait(0.01)\n",
    "    if resp is None:\n",
    "        resp = 'no_response'\n",
    "        rt = 999\n",
    "    return resp, rt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ece322-0896-4af4-aca3-1e19ec5093fc",
   "metadata": {},
   "source": [
    "- **What this code does:**\n",
    "\n",
    "> This loop continually checks for keypresses until time runs out or a response is made.\n",
    "\n",
    "> It only listens for 'a', 'd', or 'escape', which keeps the input clean and task-specific.\n",
    "\n",
    "> The trial_duration variable determines how long participants are given to make a response, and the brief 10-ms wait helps reduce CPU load by stopping PsychoPy from running the loop at full speed.\n",
    "\n",
    "- **What I liked/learned:**\n",
    "\n",
    "> I like how easy this is to read — it mirrors exactly how I’d describe the logic out loud (i.e., “keep checking until time runs out or I get a response”). The built-in timeout makes it easy to adjust task difficulty by changing one line, and using a small sleep interval keeps the program running smoothly.\n",
    "\n",
    "> I also really like how this code still saves something even when the participant doesn’t respond. Setting 'no_response' and 999 makes it clear that the trial ended because of a timeout, not because of an error in the code. It’s a simple way to keep the data complete and consistent.\n",
    "\n",
    "- **Possible improvement:**\n",
    "\n",
    "> If I were to simplify this later, I might try the event.waitKeys(timeStamped=clock, maxWait=...) method, since it does the same thing in fewer lines (https://www.psychopy.org/api/event.html#psychopy.event.waitKeys).\n",
    "\n",
    ">  But for learning purposes, this version helped me really understand how the response loop and timing work behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da9e3a-93cb-419b-a748-acec27a18b47",
   "metadata": {},
   "source": [
    "#### **(E) Centralized scoring + feedback message (keeps logic tidy)** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851b127-159f-4413-b295-04b6aa61b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Response(resp, rt, targ_there):\n",
    "    if resp == 'd' and targ_there == 1:\n",
    "        corr = 1\n",
    "        feedback = 'Correct!'\n",
    "        response_time = round(rt, 2)\n",
    "    elif resp == 'a' and targ_there == 0:\n",
    "        corr = 1\n",
    "        feedback = 'Correct!'\n",
    "        response_time = round(rt, 2)\n",
    "    elif resp == 'd' and targ_there == 0:\n",
    "        corr = 0\n",
    "        feedback = 'Incorrect'\n",
    "        response_time = round(rt, 2)\n",
    "    elif resp == 'a' and targ_there == 1:\n",
    "        corr = 0\n",
    "        feedback = 'Incorrect'\n",
    "        response_time = round(rt, 2)\n",
    "    elif resp == 'no_response':\n",
    "        corr = 0\n",
    "        feedback = 'No Response'\n",
    "        response_time = 'NA'\n",
    "    return corr, feedback, response_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd25b1-933e-4aa7-a6e8-05e0d2be0b35",
   "metadata": {},
   "source": [
    "- **What this code does:**\n",
    "\n",
    "> This function checks whether the participant’s keypress matches the correct condition and returns three things: whether they were correct, the feedback message, and their reaction time.\n",
    "\n",
    "> Having all of this in one place means I don’t have to repeat these checks throughout the script.\n",
    "\n",
    "- **What I liked/learned:**\n",
    "\n",
    "> **I like that the “is it correct?” logic is all grouped inside one dedicated function.** This is a good programming practice because it keeps the main loop focused on timing and flow while moving decision-making rules into a single, easily readable location.\n",
    "\n",
    "> **It also makes debugging much easier.** If feedback looks wrong during testing, I only need to inspect one place in the script rather than hunting through multiple blocks. I can change the response rules once and know the change applies everywhere.\n",
    "\n",
    ">  **Returning all three pieces of information together (i.e., correctness, feedback, and response time) is a tidy design choice.** It keeps my main experiment loop short, and it mirrors how real experiment frameworks (like jsPsych or PsychoPy Builder components) often structure trial outputs.\n",
    "\n",
    "> **I also learned how helpful it is to work with clear, explicit condition checks when you’re still developing the logic.** Even though this function could likely be shortened, the current version is very readable, almost self-documenting.\n",
    "\n",
    "- **What I would want to keep in mind:**\n",
    "\n",
    "> **(1)** If I ever changed which keys represent “present” and “absent,” I could easily update those lines here without needing to edit multiple sections of the script. This flexibility is helpful if the experiment is ported to a different keyboard layout or run online, where key availability likely differs.\n",
    "\n",
    "> **(2)** In a future version, I might simplify some of the repetition using a dictionary or mapping structure (e.g., mapping target conditions directly to the correct key), which would reduce the number of if/elif blocks. But for now, the explicit version is easy to follow and beginner-friendly.\n",
    "\n",
    "> **(3)** I should also think about how I’m handling missed responses. Right now, “NA” is returned as a string. Depending on my analysis plans (especially if I’m using SPSS or R later), it might be cleaner to use something like None or a numeric placeholder. But this is more of an analysis preference than a requirement.\n",
    "\n",
    "> **(4)** Overall, I would just want to remember how helpful it is to isolate logic like this. As experiments get more complex, organizing code into small, well-labelled functions becomes extremely important for maintenance, clarity, and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5755b-b009-4379-a795-314bbdec9937",
   "metadata": {},
   "source": [
    "#### **(F) Instructions + practice block + immediate feedback (good training flow)** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0668e8f-861e-4292-afd9-658955f7e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "welcome = ''''\n",
    "Welcome to the Visual Search Task\n",
    "\n",
    "You will see an assortment of shapes in different positions and orientations.\n",
    "Most will be 'L' shapes, but some may contain a 'T' shape.\n",
    "\n",
    "If the T is present, press 'd'.\n",
    "If the T is absent, press 'a'.\n",
    "\n",
    "Respond quickly!\n",
    "Press SPACE to begin 5 practice trials.\n",
    "'''\n",
    "\n",
    "instructions = visual.TextStim(win, color='white', text=welcome, units='norm', height=0.05)\n",
    "instructions.draw()\n",
    "win.flip()\n",
    "keys = event.waitKeys(keyList=['space'])\n",
    "core.wait(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba09300-62be-40b5-87ae-43c39f93ccf4",
   "metadata": {},
   "source": [
    "- **What this code does:**\n",
    "\n",
    "> This section shows participants clear, step-by-step instructions before the experiment starts.\n",
    "\n",
    "> The code displays the instructions as a single text block and waits for the participant to press SPACE before continuing.\n",
    "\n",
    "> The short core.wait(0.25) prevents accidental double keypresses.\n",
    "\n",
    "- **What I liked/learned:**\n",
    "\n",
    "> **I like that the flow is really beginner-friendly:** instructions → SPACE → short pause → practice trials.\n",
    "\n",
    "> This mirrors how real experiments would typically run, where participants get time to read and mentally prepare before the task begins.\n",
    "\n",
    "> I also like that all the task text is stored in one variable (welcome), which makes it easy to edit later without digging through the main code.\n",
    "\n",
    "- **What I would keep in mind:**\n",
    "\n",
    "> There’s a small string-quote typo at the start, but otherwise the structure seems perfect.\n",
    "\n",
    "> I might eventually move these instructions into a separate function if I wanted to show different messages for practice and real trials, but for a simple experiment, this setup is clean and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b2876-95f1-4c82-82e9-c1c357b74a37",
   "metadata": {},
   "source": [
    "#### **(G) End-of-experiment summary (avg RT, accuracy, misses)** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbcbfee-01e3-4b22-b6e8-2803dcdf0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rt = round(np.mean(rt_list), 2)\n",
    "average_corr = round(np.mean(corr_list), 2)\n",
    "total_miss = sum(miss_rt_list)\n",
    "\n",
    "avg_rt_text = f'average rt: {average_rt}'\n",
    "avg_corr_text = f'average correct: {average_corr}'\n",
    "miss_text = f'no response on {total_miss} trials'\n",
    "leave_text = 'press SPACE to exit'\n",
    "\n",
    "cor_avg_back = visual.TextStim(win, text=avg_corr_text, pos=(0, 50), height=20)\n",
    "rt_avg_back = visual.TextStim(win, text=avg_rt_text, pos=(0, -10), height=20)\n",
    "miss_tot_back = visual.TextStim(win, text=miss_text, pos=(0, -35), height=20)\n",
    "exit_text = visual.TextStim(win, text=leave_text, pos=(0, -100), height=20)\n",
    "\n",
    "cor_avg_back.draw()\n",
    "rt_avg_back.draw()\n",
    "miss_tot_back.draw()\n",
    "exit_text.draw()\n",
    "\n",
    "win.flip()\n",
    "keys = event.waitKeys(keyList=['space'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0ffaf-d5c4-4d48-96c7-7968c7e1a39a",
   "metadata": {},
   "source": [
    "- **What this code does:**\n",
    "\n",
    "> This creates a simple summary screen at the end of the experiment showing the participant’s average reaction time, accuracy, and number of missed trials.\n",
    "\n",
    ">It waits for the participant to press SPACE before closing, giving a clear endpoint to the task.\n",
    "\n",
    "- **What I liked/learned:**\n",
    "\n",
    "> I like how this provides an immediate “sanity check” for both the experimenter and the participant.\n",
    "\n",
    "> I can quickly tell if the task ran properly (for example, if reaction times are reasonable or if there were too many missed trials).\n",
    "\n",
    "> It also helped me learn how to use lists (rt_list, corr_list, miss_rt_list) to track running results across trials and summarize them cleanly at the end.\n",
    "\n",
    "- **Why this is a nice touch:**\n",
    "\n",
    "> Ending with a summary screen feels more polished and professional than just quitting the program.\n",
    "\n",
    "> It gives closure to the participant (“press SPACE to exit”) and reassures the researcher that data were recorded properly.\n",
    "\n",
    "> If I expanded this, I might add averages per condition, but as-is, it’s a really clean and informative finish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa5e1c-8f93-4fc4-89d7-6945fd7d1b74",
   "metadata": {},
   "source": [
    "#### **(H) Defining stimulus size at the beginning of the experiment** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ac7fd-9f97-4961-a054-b9ed69317cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_size = 30\n",
    "T = visual.ImageStim(win, 'Stimuli/T.png', size=stim_size)\n",
    "L = visual.ImageStim(win, 'Stimuli/L.png', size=stim_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e3ac0-bfa9-47da-ac32-df0a2ef5bd87",
   "metadata": {},
   "source": [
    "- **What this code does:**\n",
    "\n",
    "> This defines the size of the target (T) and distractor (L) images at the very start of the script.\n",
    "\n",
    "> Both images pull from the same stim_size variable, which makes the display consistent and easy to adjust later.\n",
    "\n",
    "- **What I liked/learned:**\n",
    "\n",
    "> Instead of manually changing the image size in multiple places, I can adjust all stimuli by changing one value at the top.\n",
    "\n",
    "> This keeps the code cleaner and reduces errors if I ever need to resize the stimuli for a different display resolution or participant setup.\n",
    "\n",
    "- **What I would have to change experiment to experiment:**\n",
    "\n",
    "> If the experiment is moved to a different monitor or scaled for online testing, I only have to tweak stim_size once.\n",
    "\n",
    "> My code doesn’t use a shared size variable, so resizing would mean updating several separate lines.\n",
    "\n",
    "> However, if different stim needs different sizes, this method of defining the size once may not be a good idea. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432a507-5a14-4df2-8ec0-f5aac5939bfd",
   "metadata": {},
   "source": [
    "## **(3) Things I could show my classmate** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39c7c1-613e-4fa5-bb46-5bff920830ad",
   "metadata": {},
   "source": [
    "#### **(A) Explicit quit condition inside the response block** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3528b6d-af33-46b4-93a2-fbe452b8ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"escape\" in response:\n",
    "    win.close()\n",
    "    core.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de2cfc-8664-4e48-8985-3578f27700a6",
   "metadata": {},
   "source": [
    "- This gives a safe, graceful exit at any point in the task.\n",
    "\n",
    "- Their version includes an escape condition but buries it inside another function, which makes it less transparent (i.e., this is from my understanding, and I could be wrong).\n",
    "\n",
    "- It is possible that my approach is easier to spot and modify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880db61-75ac-42dc-9d71-bd8c53b9f4da",
   "metadata": {},
   "source": [
    "#### **(B) Self-contained logic per trial (i.e., less global dependence)** ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8880b-b9a4-45f6-9261-e51909715326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targ_pres(trial_list, total_trials, distract, target, condition_index):\n",
    "    pres_or_not = random.choice(trial_list)\n",
    "    trial_list.remove(pres_or_not)\n",
    "    \n",
    "    if pres_or_not <= np.median(total_trials):\n",
    "        targ_there = 0\n",
    "        stimuli = pos_and_ori(distract, distract, condition_index)\n",
    "    else:\n",
    "        targ_there = 1\n",
    "        stimuli = pos_and_ori(target, distract, condition_index)\n",
    "    return targ_there, stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc0116-5f26-4857-9c56-bfead4f80456",
   "metadata": {},
   "source": [
    "- **In my code, most variables (like Set_Size, present, Coordinates_used, and target_position) are defined *inside* the trial loop.**\n",
    "\n",
    "> This means each trial is self-contained and resets all of its values from scratch. I liked this approach because it made the trial structure easier for me to follow logically — everything needed for that one trial is right there in the loop.\n",
    "\n",
    "> If something ever goes wrong (like a missed stimulus or crash), it doesn’t affect the next trial since nothing carries over globally.\n",
    "\n",
    "- **In contrast, this section of my classmate’s code defines the logic for whether a target is present or absent using shared lists (trial_list and total_trials) that get modified each time the function runs.**\n",
    "\n",
    "> That works just fine, but from my understanding, it would mean that the function depends on those global variables staying in sync.\n",
    "\n",
    "> For example, removing items from trial_list (trial_list.remove(pres_or_not)) changes its length across trials, so if something breaks mid-run, it could make the block go out of sync or cause inconsistencies, which would make debugging harder later on.\n",
    "\n",
    "- **As mentioned previously, I really liked that my classmate wrapped this decision logic in a function — that’s very efficient — but I’d personally move the key trial variables into the main trial loop rather than passing them in from outside.**\n",
    "\n",
    "> Doing so would make it easier to reuse the code in another experiment later, since each trial would generate its own state independently instead of depending on global lists or counters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b5cce-20c7-4162-aba5-c760e5483487",
   "metadata": {},
   "source": [
    "#### **(C) Explicit reaction-time clock for the main experimental trials** ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab29617-0c8c-4a0b-aef4-4cbf62ff2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_clock = core.Clock()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466928eb-9760-4f4a-a7b7-01b591f5e897",
   "metadata": {},
   "source": [
    "- This makes it obvious that RTs are measured only during the main trials, which, in my opinion, is a good practice.\n",
    "\n",
    "- However, unless you want to show a participant on the practice trials how fast or slow they were, or something along those lines (i.e., it depends on the task). Possibly to motivate participants to complete the task faster?\n",
    "  \n",
    "- My classmate calculated RTs inside the key loop using core.getTime( ) - startTime, which works but is a bit less organized (I could be so wrong about this, but the other way seemed more intuitive).\n",
    "\n",
    "- Having a dedicated clock object makes timing cleaner and easier to expand later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e756a-815c-4d42-ba11-c13efef13d10",
   "metadata": {},
   "source": [
    "## **(4) Questions/thoughts I still have about inputting \"Trials per condition\"** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55c619-8c28-4874-9b20-3894f46868ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlg.addField('Trials Per Cond:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b35c288-7271-4161-a041-cac6c5943d28",
   "metadata": {},
   "source": [
    "- **I understand that this lets the experimenter type how many trials they want for each condition, but I’m still a bit unsure where this number actually gets used in the script.**\n",
    "\n",
    "> Does it directly control the total number of trials, or does it feed into another loop or function later on?\n",
    "\n",
    "- **I’m wondering why the experimenter needs to input this value manually rather than having it pre-defined in the code.**\n",
    "\n",
    "> Is this meant mainly for flexibility during testing, or is there an experimental reason to keep it adjustable?\n",
    "\n",
    "- **I’m also curious whether participants should ever see or be aware of this number.**\n",
    "\n",
    "> Since it’s entered in the dialog box before the experiment starts, it seems designed for the experimenter, not the participant — but I’d like to confirm that’s standard practice.\n",
    "\n",
    "- **Finally, I’d like to understand how “Trials Per Condition” relates to total trial count.**\n",
    "\n",
    "> For example, if there are multiple set sizes or present/absent conditions, does PsychoPy automatically multiply the entered number across conditions, or does that need to be specified somewhere else?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
