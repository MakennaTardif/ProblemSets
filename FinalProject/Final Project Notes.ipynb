{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4c3590-447f-4d0e-bf3d-1010395a1489",
   "metadata": {},
   "source": [
    "# **Part 1: Coding my Experiment** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83aea2-6db4-49d8-9aa2-f36735b855ab",
   "metadata": {},
   "source": [
    "## **Step 1: Getting the face stimuli prepared** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d8f70-f9fa-4e98-9800-ca5a0abf2a96",
   "metadata": {},
   "source": [
    "#### **What do I need?**\n",
    "- 6 trials with 6 different identities per trial (i.e., 3 female trials and 3 male trials)**\n",
    "- This means that in this part of the experiment, each participant will initially view 36 faces\n",
    "\n",
    "#### **Subject Groups:**\n",
    "- **There are also 3 subject groups where each identity and emotion rotates**\n",
    "    - **Example:** Bob = angry in Group 1, happy in Group 2, neutral in Group 3.\n",
    "    - This means that every identity must have all 3 emotion versions, even though each participant only sees one of them.\n",
    "- **So if there are 36 identities in the grid phase, that’s:**\n",
    "    - **36 identities × 3 emotions =** 108 images\n",
    "\n",
    "#### **Naming system for stim files for the grid phase:**\n",
    "- *Emotion + Gender + stim #*\n",
    " - **Example file names:** AngryFemale1, HappyMale18, NeutralFemale6\n",
    "      \n",
    "#### **Naming system for stim files for the memory task:**\n",
    "- Regular naming system for previously seen faces\n",
    "- **New faces:** *Emotion + Gender + Foil + stim #*\n",
    "- **Example file names:** AngryMaleFoil2, HappyFemaleFoil1, NeutralMaleFoil3\n",
    "- I have 3 new identities for each gender (i.e., 3 x 2 x 3 = total of 18 foils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4c22c-33f7-4189-8928-8a86a2dab0a4",
   "metadata": {},
   "source": [
    "## **Step 2: One-time stimulus resizing** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad8f74-5a30-491d-98ee-2b831772aa82",
   "metadata": {},
   "source": [
    "- The original face stimuli were very large (i.e., 2444 × 1718 px), which is far too large to display on the screen.\n",
    "  \n",
    "- PsychoPy loads the full-resolution image into memory even if it is displayed smaller, which would dramatically increase memory usage and slow down the experiment (i.e., I learned the hard way....).\n",
    "\n",
    "- Because the task requires six faces to be displayed simultaneously, using large images would risk frame drops, slow loading, and poor performance, especially if we ran this experiment online (e.g., Pavlovia).\n",
    "\n",
    "- The images were resized to 240 × 180 pixels to ensure fast loading, smooth presentation, and consistent visual quality across trials without altering the recognizability of the faces.\n",
    "\n",
    "- Using the code allowed me to resize the images once rather than manually per image.\n",
    "\n",
    "- I am very happy that I know how to do this now, as it will be very handy for future studies!\n",
    "\n",
    "- I have submitted this code separately instead of commenting it out in my main experiment code (i.e., it kept confusing me when I originally had just commented it out). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c227b-b9ec-4521-89a9-e8ab8771746e",
   "metadata": {},
   "source": [
    "## **Step 3: Preloading images** ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "202fbc55-1dcd-4e78-8cfe-2ba5040da8f1",
   "metadata": {},
   "source": [
    "### **Why did I preload stim images by parsing filenames (instead of using a CSV)?** ###\n",
    "\n",
    "- **Challenge** - First of all, I wanted to challenge myself to do something I have not done yet.\n",
    "\n",
    "- **No duplication of information** — Emotion, gender, and identity are already encoded in the filenames, so I don’t need a second file that repeats the same metadata.\n",
    "\n",
    "- **Eliminates sync errors** — If I add, delete, or rename an image, the code automatically updates; a CSV would need manual edits and could easily become outdated.\n",
    "\n",
    "- **Prevents labelling mistakes** — The regex extracts the correct emotion/gender from the filename, so I cannot accidentally mislabel a face in a CSV.\n",
    "\n",
    "- **Scales automatically** — Whether I have 36 images or 300, the code reads everything in the folder without any extra work.\n",
    "\n",
    "- **Cleaner workflow for development** — I can focus on stimuli and experimental logic instead of maintaining a separate metadata file.\n",
    "\n",
    "- **I can still add a CSV later** - This is if I need conceptual info not encoded in filenames (e.g., group rotation mapping or foil flags).\n",
    "\n",
    "- **Links:**\n",
    "\n",
    "  > https://www.psychopy.org/api/visual/imagestim.html\n",
    "  >\n",
    "  > https://softwareengineering.stackexchange.com/questions/386702/having-a-flag-to-indicate-if-we-should-throw-errors/386763#386763"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04815331-3d49-4f37-92ab-ecf32a6ed127",
   "metadata": {},
   "source": [
    "## **Step 4: Programming the Experiment** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47635a2b-65fd-4ae3-bd42-6229c95440d6",
   "metadata": {},
   "source": [
    "### **4.1) Grid and slider task** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391f6f9-7a71-4d4a-aa8b-5a30fffae52e",
   "metadata": {},
   "source": [
    "#### **Subject group assignment (i.e., counterbalancing across participants):**\n",
    "\n",
    "- I assign each participant to a **subject group (1, 2, or 3)** so that the same identities are not always shown with the same emotional expression.\n",
    "    - This is important because some faces may naturally look more intense or distinctive, which could otherwise confound emotion effects.\n",
    "- By counterbalancing emotion–identity pairings across groups, any identity-specific bias is spread evenly across conditions.\n",
    "- The subject group is saved directly into the **CSV file**, so I always know which counterbalancing version a participant completed.\n",
    "    - This makes the design transparent, reproducible, and easy to describe in the Methods section.\n",
    "\n",
    "#### **Emotion rotation logic (i.e., base sets → actual emotions):**\n",
    "\n",
    "- I do **not** randomly assign emotions face-by-face.\n",
    "- Instead, I define three **base sets** (0, 1, 2) and then map those base sets to emotions depending on the participant’s subject group.\n",
    "- **Example:**\n",
    "  - Group 1: base0 → angry, base1 → happy, base2 → neutral\n",
    "  - Group 2: base0 → happy, base1 → neutral, base2 → angry\n",
    "  - Group 3: base0 → neutral, base1 → angry, base2 → happy\n",
    "- **This rotation ensures:**\n",
    "  - Each identity appears equally often as angry, happy, and neutral across participants.\n",
    "  - Within a single participant, emotion assignment is systematic and controlled rather than random.\n",
    "- This approach is critical for separating **emotion-based biases** from **identity-based effects**.\n",
    "\n",
    "\n",
    "#### **Trial construction using base sets (i.e., guaranteeing 2–2–2 per grid):**\n",
    "\n",
    "- Each grid always contains **exactly six faces**.\n",
    "- **For every trial, I include:**\n",
    "  - 2 identities from base set 0\n",
    "  - 2 identities from base set 1\n",
    "  - 2 identities from base set 2\n",
    "- **Because each base set maps onto exactly one emotion for a given subject group, this guarantees:**\n",
    "  - 2 angry faces\n",
    "  - 2 happy faces\n",
    "  - 2 neutral faces\n",
    "- **This is important because:**\n",
    "  - The true proportion is always **33.33% per emotion**, which provides a stable baseline.\n",
    "  - Any deviation in estimates reflects **perceptual or cognitive bias**, not changes in stimulus composition.\n",
    "\n",
    "\n",
    "#### **Gender separation across trials (i.e., 3 male, 3 female):**\n",
    "\n",
    "- I construct **three trials with only female faces** and **three trials with only male faces**.\n",
    "- This avoids mixing genders within a single grid, which could introduce additional variability or attentional effects.\n",
    "- **Gender can be examined later as:**\n",
    "  - A control variable, **OR** a factor of interest if needed.\n",
    "- **Each trial’s gender is explicitly logged in the CSV:**\n",
    "  - As a numeric code (1 = male, 2 = female), and as a readable label (“male” / “female”).\n",
    "\n",
    "\n",
    "#### **Fixation cross before each grid:**\n",
    "\n",
    "- A fixation cross is shown for **500 ms** before each grid.\n",
    "    - This centers visual attention, clears residual attention from the previous slider screen, and standardizes the pre-stimulus interval across trials.\n",
    "\n",
    "\n",
    "#### **Grid display:**\n",
    "\n",
    "- Each grid is displayed for **2000 ms**.\n",
    "- **This duration is:** Long enough to perceive the entire set of faces, and short enough to discourage deliberate counting strategies.\n",
    "- Faces are arranged **equally spaced in a circle**:\n",
    "  - Avoids positional biases (e.g., always putting angry faces in the same location)\n",
    "  - Visually resembles a “crowd,” matching the theoretical framing of the task.\n",
    "- The order of faces around the circle is randomized on every trial.\n",
    "\n",
    "\n",
    "#### **Slider-based proportion estimates:**\n",
    "\n",
    "- After the grid disappears, participants see **three sliders at the same time**:\n",
    "  - Angry (%)\n",
    "  - Happy (%)\n",
    "  - Neutral (%)\n",
    "- Sliders range from **0 to 100** in **steps of 5**.\n",
    "- **Showing all three sliders simultaneously:**\n",
    "  - Encourages participants to think in terms of proportions, and makes trade-offs between categories explicit.\n",
    "- Participants cannot continue until the three sliders **sum to exactly 100%**.\n",
    "    - This ensures responses are directly interpretable as proportions, and prevents unusable data (e.g., totals above or below 100).\n",
    "\n",
    "\n",
    "#### **Why did I enforce the “sum to 100%” rule?**\n",
    "\n",
    "- **Without this rule, participants could give:**\n",
    "  - inconsistent totals,\n",
    "  - estimates that cannot be compared across trials or participants.\n",
    "- **Enforcing the rule:**\n",
    "  - **(1)** Keeps the data clean,\n",
    "  - **(2)** Eliminates the need for post-hoc normalization,\n",
    "  - **(3)** Aligns directly with the theoretical question about **relative frequency judgments**.\n",
    "- Participants retain full control over their responses, and the constraint only ensures validity.\n",
    "\n",
    "\n",
    "#### **Event clearing and response control:**\n",
    "\n",
    "- I clear keyboard events before the slider screen appears.\n",
    "    - This prevents accidental carryover (e.g., pressing space to end the grid and immediately submitting the sliders).\n",
    "- This step avoids unintended responses and improves data quality.\n",
    "    - I learned the hard way with this and needed to problem-solve.\n",
    "\n",
    "\n",
    "#### **Trial-by-trial data logging:**\n",
    "\n",
    "- **For each trial, I save:**\n",
    "  - The participant index, subject group, trial number, angry, happy, and neutral estimates, trial gender (numeric code and label), identities shown on that trial, emotion versions shown, date/time of the response.\n",
    "- This makes each row of the CSV **self-contained**.\n",
    "- The dataset can be analyzed directly in SPSS or R without additional restructuring.\n",
    "\n",
    "\n",
    "#### **Tracking identities for later tasks:**\n",
    "\n",
    "- I keep a separate record of the **36 unique identities** shown across the grid task.\n",
    "- **This is essential for later phases:**\n",
    "  - (1) Memory task\n",
    "  - (2) Emotion rating task\n",
    "- **Because identity–emotion pairings are counterbalanced, this record ensures:**\n",
    "  - The same faces can be presented again accurately\n",
    "  - Foils can be selected correctly\n",
    "  - Follow-up tasks remain consistent with the grid phase.\n",
    "\n",
    "\n",
    "#### **Overall rationale:**\n",
    "\n",
    "- **The structure of this task is designed to:**\n",
    "  - Tightly control stimulus composition, isolate emotion-based frequency biases, and produce clean, interpretable proportion estimates.\n",
    "- Every design choice (counterbalancing, fixed composition, sliders, timing) is there to reduce noise and increase confidence that any bias reflects **how people summarize emotional information**, not artifacts of the stimulus set or response format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45766d1-da27-438a-8ef7-c76e1fa5a54b",
   "metadata": {},
   "source": [
    "### **4.2) Memory task** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30538f74-f23f-4226-86b6-9fac615576f5",
   "metadata": {},
   "source": [
    "#### **Purpose of the memory task**\n",
    "- This section measures **recognition memory** for the exact face images participants just viewed in the grid task.\n",
    "- Faces are shown **one at a time**, and participants make a **YES / NO** judgment about whether they saw that exact picture earlier.\n",
    "- **The memory task includes:**\n",
    "  - **36 OLD faces** (all faces actually shown during the grid task)\n",
    "  - **18 FOILS** (new faces not shown before; 9 male, 9 female)\n",
    "- This ratio keeps the task reasonably short while still allowing reliable estimates of **hits, misses, false alarms, and correct rejections**.\n",
    "\n",
    "#### **Why does the memory task come after the grid task?**\n",
    "- The grid task determines **exactly which faces** each participant sees.\n",
    "- These faces are stored during the grid task in `seen_faces`.\n",
    "- The memory task must occur *after* the grid task so it can be built from the participant’s **actual exposure**, not a predefined list.\n",
    "\n",
    "#### **Building in the OLD memory trials**\n",
    "- OLD trials are created directly from `seen_faces`.\n",
    "- **For each face shown in the grid task, the memory trial stores:**\n",
    "  - `identity` (e.g., F12, M7)\n",
    "  - `gender` (male / female)\n",
    "  - `emotion` (the exact emotion version shown to that participant)\n",
    "  - `old = 1` to mark it as previously seen\n",
    "  - the corresponding PsychoPy `ImageStim`\n",
    "- This ensures memory is tested for the **exact picture**, not just the identity.\n",
    "\n",
    "#### **Why are foils handled separately?**\n",
    "- Foil images include `\"Foil\"` in their filenames.\n",
    "- The preload regex intentionally **excludes foils**, so they are not added to `image_cache`.\n",
    "- The memory task therefore, explicitly scans for foil files and loads them separately.\n",
    "- This avoids accidental inclusion of foils during the grid task.\n",
    "\n",
    "#### **Selecting 18 foils (i.e., gender balanced)**\n",
    "- All available foils are shuffled first.\n",
    "- **From these, 18 foils are selected:**\n",
    "  - 9 male\n",
    "  - 9 female\n",
    "- This prevents gender imbalance in the new (foil) items.\n",
    "- The final foil list is shuffled again before combining with old items.\n",
    "\n",
    "#### **Creating the final memory trial list**\n",
    "- **OLD and FOIL trials are combined:**\n",
    "  - `memory_trials = old_trials + foil_trials`\n",
    "- The combined list is shuffled so OLD and FOIL faces are mixed.\n",
    "- **Total memory trials =** 54 (i.e., 36 old + 18 foils).\n",
    "\n",
    "#### **Reusing a single ImageStim for memory trials**\n",
    "- A single `ImageStim` (`memory_face`) is created with `image=None`.\n",
    "- **On each trial, the image is updated dynamically:**\n",
    "  - `memory_face.image = trial[\"stim\"].image`\n",
    "- This is more efficient than creating a new `ImageStim` each trial and ensures consistent size and position.\n",
    "\n",
    "#### **Response method: YES / NO buttons**\n",
    "- Participants respond using **on-screen buttons**, clicked with the mouse.\n",
    "- This avoids keyboard mappings and makes responses intuitive.\n",
    "- Reaction time is **not recorded**, keeping the task focused on accuracy rather than speed.\n",
    "\n",
    "#### **Memory trial flow**\n",
    "- **For each memory trial:**\n",
    "  - Display the question prompt\n",
    "  - Display the face image\n",
    "  - Display YES and NO buttons\n",
    "  - Wait until the participant clicks one button\n",
    "- Mouse clicks are checked using `mouse.isPressedIn()`.\n",
    "\n",
    "#### **Computing accuracy**\n",
    "- **Each trial includes a ground-truth label:**\n",
    "  - `old = 1` → face was shown earlier\n",
    "  - `old = 0` → foil (new face)\n",
    "- **Correctness is computed as:**\n",
    "  - YES on old → correct (Hit)\n",
    "  - NO on old → incorrect (Miss)\n",
    "  - YES on foil → incorrect (False Alarm)\n",
    "  - NO on foil → correct (Correct Rejection)\n",
    "- **This is coded as:**\n",
    "  - `correct = int(response == trial[\"old\"])`\n",
    "\n",
    "#### **Saving memory data in the same CSV as grid data**\n",
    "- Both tasks are saved to the **same CSV file**.\n",
    "- **A `\"task\"` column distinguishes rows:**\n",
    "  - `\"grid\"` for grid/slider trials\n",
    "  - `\"memory\"` for memory trials\n",
    "- **For memory rows:**\n",
    "  - Grid-related columns are left blank\n",
    "  - Memory-specific columns are filled:\n",
    "    - `memory_identity`\n",
    "    - `memory_gender`\n",
    "    - `memory_emotion`\n",
    "    - `old`\n",
    "    - `response`\n",
    "    - `correct`\n",
    "- This format keeps the data **long-form** and analysis-ready.\n",
    "\n",
    "#### **What does this memory task allow for later?**\n",
    "- **Calculation of:**\n",
    "  - Hit rate\n",
    "  - False alarm rate\n",
    "  - Overall accuracy\n",
    "  - Signal detection measures (d′, C)\n",
    "- Because OLD and FOIL trials are clearly coded, these analyses are straightforward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aface7d3-3d3d-4d94-9722-dbe2e4870daa",
   "metadata": {},
   "source": [
    "### **4.3) Emotion rating task** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a1cfe-7f69-45b0-aa55-8e368327aef6",
   "metadata": {},
   "source": [
    "#### **Task Overview**\n",
    "- After completing the grid estimation and memory tasks, participants completed an **emotion rating task**.\n",
    "- This task was presented last so that explicit judgments of emotional negativity would not influence earlier frequency estimates or memory decisions.\n",
    "\n",
    "#### **Stimuli**\n",
    "- Only the **36 faces previously shown during the grid task** were included.\n",
    "- **Foil faces were not shown** in this task.\n",
    "- Each face was presented **one at a time**, using the same emotion version that the participant had seen earlier (angry, happy, or neutral), as determined by subject-group counterbalancing.\n",
    "\n",
    "#### **Procedure:**\n",
    "**On each trial:**\n",
    "1. A single face appeared at the center of the screen.\n",
    "2. Participants rated **how negative the face’s emotion appeared** using a **9-point rating scale** displayed directly beneath the face.\n",
    "3. **The scale ranged from:**\n",
    "   - **1 = Very positive**\n",
    "   - **5 = Neutral**\n",
    "   - **9 = Very negative**\n",
    "4. Participants select a rating using a slider and press **SPACE** to continue to the next face.\n",
    "5. A response is required before advancing to the next trial.\n",
    "\n",
    "- The order of faces was **randomized** across participants.\n",
    "\n",
    "#### **Data Recording:**\n",
    "- **For each rating trial, the following variables were saved to the same CSV file as the grid and memory tasks:**\n",
    "    - `rating_identity`: Identity code of the face being rated  \n",
    "    - `rating_gender`: Gender of the face  \n",
    "    - `rating_emotion`: Emotion version shown (angry, happy, neutral)  \n",
    "    - `negativity_rating`: Participant’s rating on the 1–9 scale\n",
    "      \n",
    "- All grid- and memory-specific columns were left blank for rating trials, allowing all tasks to be stored in a **single, unified data file**.\n",
    "\n",
    "#### **Planned Use:**\n",
    "- **Negativity ratings can be averaged separately for:**\n",
    "    - Angry faces  \n",
    "    - Happy faces  \n",
    "    - Neutral faces\n",
    "- **Overall:** These ratings provide an **independent manipulation check** AND allow examination of how perceived emotional negativity relates to frequency estimation and memory performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PsychoPy",
   "language": "python",
   "name": "psychopy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
